{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas dataframe\n",
    "df = pd.read_csv(r\"C:\\Users\\91939\\OneDrive\\Desktop\\Copper_P4\\DATASET\\INDUSTRY_COPPER.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['item_date'].unique())) #\n",
    "print(len(df['customer'].unique())) #\n",
    "print(len(df['material_ref'].unique()))\n",
    "print(len(df['product_ref'].unique())) #\n",
    "print(len(df['delivery date'].unique())) # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "missing_values_count = df.isnull().sum()\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with data in wrong format\n",
    "# for categorical variables, this step is ignored\n",
    "# df = df[df['status'].isin(['Won', 'Lost'])]\n",
    "df['item_date'] = pd.to_datetime(df['item_date'], format='%Y%m%d', errors='coerce').dt.date\n",
    "df['quantity tons'] = pd.to_numeric(df['quantity tons'], errors='coerce')\n",
    "df['customer'] = pd.to_numeric(df['customer'], errors='coerce')\n",
    "df['country'] = pd.to_numeric(df['country'], errors='coerce')\n",
    "df['application'] = pd.to_numeric(df['application'], errors='coerce')\n",
    "df['thickness'] = pd.to_numeric(df['thickness'], errors='coerce')\n",
    "df['width'] = pd.to_numeric(df['width'], errors='coerce')\n",
    "df['material_ref'] = df['material_ref'].str.lstrip('0')\n",
    "df['product_ref'] = pd.to_numeric(df['product_ref'], errors='coerce')\n",
    "df['delivery date'] = pd.to_datetime(df['delivery date'], format='%Y%m%d', errors='coerce').dt.date\n",
    "df['selling_price'] = pd.to_numeric(df['selling_price'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = df.isnull().sum()\n",
    "print(missing_values_count)\n",
    "print(df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# material_ref has large set of null values, so replacing them with unknown \n",
    "df['material_ref'].fillna('unknown', inplace=True)\n",
    "# deleting the remaining null values as they are less than 1% of data which can be neglected\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = df.isnull().sum()\n",
    "print(missing_values_count)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['quantity tons','selling_price','application','thickness','width', 'country']\n",
    "# ['status','item type']\n",
    "\n",
    "df_p['quantity tons']   # skewed\n",
    "df_p['country']\n",
    "df_p['application']\n",
    "df_p['thickness']       # skewed\n",
    "df_p['width']           \n",
    "df_p['selling_price']   # skewed\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.distplot(df_p['quantity tons'])\n",
    "plt.show()\n",
    "sns.distplot(df_p['country'])\n",
    "plt.show()\n",
    "sns.distplot(df_p['application'])\n",
    "plt.show()\n",
    "sns.distplot(df_p['thickness'])\n",
    "plt.show()\n",
    "sns.distplot(df_p['width'])\n",
    "plt.show()\n",
    "sns.distplot(df_p['selling_price'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mask1 = df_p['selling_price'] <= 0\n",
    "print(mask1.sum())\n",
    "df_p.loc[mask1, 'selling_price'] = np.nan\n",
    "\n",
    "mask1 = df_p['quantity tons'] <= 0\n",
    "print(mask1.sum())\n",
    "df_p.loc[mask1, 'quantity tons'] = np.nan\n",
    "\n",
    "mask1 = df_p['thickness'] <= 0\n",
    "print(mask1.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.dropna(inplace=True)\n",
    "len(df_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_p['selling_price_log'] = np.log(df_p['selling_price'])\n",
    "sns.distplot(df_p['selling_price_log'])\n",
    "plt.show()\n",
    "\n",
    "df_p['quantity tons_log'] = np.log(df_p['quantity tons'])\n",
    "sns.distplot(df_p['quantity tons_log'])\n",
    "plt.show()\n",
    "\n",
    "df_p['thickness_log'] = np.log(df_p['thickness'])\n",
    "sns.distplot(df_p['thickness_log'])\n",
    "plt.show()\n",
    "# reverts log\n",
    "# df_p['reverted_values'] = np.exp(df_p['selling_price_log'])\n",
    "# sns.distplot(df_p['reverted_values'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df_p[['quantity tons_log','application','thickness_log','width','selling_price_log','country','customer','product_ref']].corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(x, annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X=df_p[['quantity tons_log','status','item type','application','thickness_log','width','country','customer','product_ref']]\n",
    "y=df_p['selling_price_log']\n",
    "# encoding categorical variables\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(X[['item type']])\n",
    "X_ohe = ohe.fit_transform(X[['item type']]).toarray()\n",
    "ohe2 = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe2.fit(X[['status']])\n",
    "X_be = ohe2.fit_transform(X[['status']]).toarray()\n",
    "# independent features after encoding\n",
    "X = np.concatenate((X[['quantity tons_log', 'application', 'thickness_log', 'width','country','customer','product_ref']].values, X_ohe, X_be), axis=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# test and train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "# decision tree\n",
    "dtr = DecisionTreeRegressor()\n",
    "# hyperparameters\n",
    "param_grid = {'max_depth': [2, 5, 10, 20],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'max_features': ['auto', 'sqrt', 'log2']}\n",
    "# gridsearchcv\n",
    "grid_search = GridSearchCV(estimator=dtr, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "# evalution metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('Mean squared error:', mse)\n",
    "print('R-squared:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['quantity tons_log', 'application', 'thickness_log', 'width','country','customer','product_ref']].values, X_ohe, X_be\n",
    "new_sample = np.array([[np.log(40), 10, np.log(250), 0, 28,30202938,1670798778,'PL','Won']])\n",
    "new_sample_ohe = ohe.transform(new_sample[:, [7]]).toarray()\n",
    "new_sample_be = ohe2.transform(new_sample[:, [8]]).toarray()\n",
    "new_sample = np.concatenate((new_sample[:, [0,1,2, 3, 4, 5, 6,]], new_sample_ohe, new_sample_be), axis=1)\n",
    "new_sample1 = scaler.transform(new_sample)\n",
    "new_pred = best_model.predict(new_sample1)\n",
    "print('Predicted selling price:', np.exp(new_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "import pickle\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open('t.pkl', 'wb') as f:\n",
    "    pickle.dump(ohe, f)\n",
    "with open('s.pkl', 'wb') as f:\n",
    "    pickle.dump(ohe2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_p))\n",
    "df_p.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = df_p[df_p['status'].isin(['Won', 'Lost'])]\n",
    "len(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder,LabelBinarizer\n",
    "\n",
    "Y = df_c['status']\n",
    "X= df_c[['quantity tons_log','selling_price_log','item type','application','thickness_log','width','country','customer','product_ref']]\n",
    "\n",
    "# encoding categorical variables\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(X[['item type']])\n",
    "X_ohe = ohe.fit_transform(X[['item type']]).toarray()\n",
    "be = LabelBinarizer()\n",
    "be.fit(Y) \n",
    "y = be.fit_transform(Y)\n",
    "# independent features after encoding\n",
    "X = np.concatenate((X[['quantity tons_log', 'selling_price_log','application', 'thickness_log', 'width','country','customer','product_ref']].values, X_ohe), axis=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# decision tree classifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Confusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalution Metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "# ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the status for a new sample\n",
    "# 'quantity tons_log', 'selling_price_log','application', 'thickness_log', 'width','country','customer','product_ref']].values, X_ohe\n",
    "new_sample = np.array([[np.log(700), np.log(956), 10, np.log(2),1500,28.0,30202938,1670798778,'W']])\n",
    "new_sample_ohe = ohe.transform(new_sample[:, [8]]).toarray()\n",
    "new_sample = np.concatenate((new_sample[:, [0,1,2, 3, 4, 5, 6,7]], new_sample_ohe), axis=1)\n",
    "new_sample = scaler.transform(new_sample)\n",
    "new_pred = dtc.predict(new_sample)\n",
    "if new_pred==1:\n",
    "    print('The status is: Won')\n",
    "else:\n",
    "    print('The status is: Lost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "import pickle\n",
    "with open('cmodel.pkl', 'wb') as file:\n",
    "    pickle.dump(dtc, file)\n",
    "with open('cscaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open('ct.pkl', 'wb') as f:\n",
    "    pickle.dump(ohe, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
